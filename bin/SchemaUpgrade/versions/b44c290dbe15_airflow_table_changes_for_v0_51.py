"""Airflow table changes for v0.51

Revision ID: b44c290dbe15
Revises: 5d9908c8cf55
Create Date: 2019-06-17 06:26:14.601969

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import mysql
from sqlalchemy import Enum

# revision identifiers, used by Alembic.
revision = 'b44c290dbe15'
down_revision = '5d9908c8cf55'
branch_labels = None
depends_on = None


def upgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table('airflow_import_task_execution')
    op.drop_table('airflow_import_dag_execution')
    op.drop_table('airflow_execution_type')
    op.add_column('airflow_custom_dags', sa.Column('auto_regenerate_dag', mysql.TINYINT(display_width=4), server_default=sa.text("'1'"), nullable=False, comment='1 = The DAG will be auto regenerated by manage command'))
    op.alter_column('airflow_custom_dags', 'operator_notes',
               existing_type=mysql.TEXT(),
               comment='Free text field to write a note about the custom DAG. ',
               existing_comment='Free text field to write a note about the import. ',
               existing_nullable=True)
    op.alter_column('airflow_custom_dags', 'retries',
               existing_type=mysql.INTEGER(display_width=11),
               comment='How many retries should be Task do in Airflow before it failes',
               existing_nullable=False,
               existing_server_default=sa.text("'0'"))
    op.create_table_comment(
        'airflow_custom_dags',
        'Its possible to construct a DAG that have no import, export or ETL definitions in it, but instead just Tasks from the airflow_task table. That might nbe useful to for example run custom Hive Code after an import is completed as a separate DAG. Defining a DAG in here also requires you to have at least one task in airflow_tasks defined "in main"',
        existing_comment=None,
        schema=None
    )
    op.create_table_comment(
        'airflow_dag_sensors',
        'This table is used only in Legacy DBImport',
        existing_comment=None,
        schema=None
    )
    op.add_column('airflow_etl_dags', sa.Column('auto_regenerate_dag', mysql.TINYINT(display_width=4), server_default=sa.text("'1'"), nullable=False, comment='1 = The DAG will be auto regenerated by manage command'))
    op.alter_column('airflow_etl_dags', 'filter_job',
               existing_type=mysql.VARCHAR(length=64),
               comment='Filter string for JOB in etl_jobs table',
               existing_nullable=False)
    op.alter_column('airflow_etl_dags', 'filter_source_db',
               existing_type=mysql.VARCHAR(length=256),
               comment='Filter string for SOURCE_DB in etl_jobs table',
               existing_nullable=True)
    op.alter_column('airflow_etl_dags', 'filter_target_db',
               existing_type=mysql.VARCHAR(length=256),
               comment='Filter string for TARGET_DB in etl_jobs table',
               existing_nullable=True)
    op.alter_column('airflow_etl_dags', 'filter_task',
               existing_type=mysql.VARCHAR(length=64),
               comment='Filter string for TASK in etl_jobs table',
               existing_nullable=True)
    op.alter_column('airflow_etl_dags', 'operator_notes',
               existing_type=mysql.TEXT(),
               comment='Free text field to write a note about the ETL job. ',
               existing_comment='Free text field to write a note about the import. ',
               existing_nullable=True)
    op.alter_column('airflow_etl_dags', 'retries',
               existing_type=mysql.TINYINT(display_width=4),
               comment='How many retries should be Task do in Airflow before it failes',
               existing_nullable=True)
    op.alter_column('airflow_etl_dags', 'trigger_dag_on_success',
               existing_type=mysql.VARCHAR(length=64),
               comment='<NOT USED>',
               existing_nullable=True)
    op.create_table_comment(
        'airflow_etl_dags',
        'To create a DAG in Airflow for only ETL jobs, this is the table that holds all definitions of the DAG configuration, including the filter that defines what ETL jobs to run, schedules times, pool names and much more. ',
        existing_comment=None,
        schema=None
    )
    op.add_column('airflow_export_dags', sa.Column('auto_regenerate_dag', mysql.TINYINT(display_width=4), server_default=sa.text("'1'"), nullable=False, comment='1 = The DAG will be auto regenerated by manage command'))
    op.alter_column('airflow_export_dags', 'operator_notes',
               existing_type=mysql.TEXT(),
               comment='Free text field to write a note about the export. ',
               existing_comment='Free text field to write a note about the import. ',
               existing_nullable=True)
    op.alter_column('airflow_export_dags', 'retries',
               existing_type=mysql.TINYINT(display_width=4),
               comment='How many retries should be Task do in Airflow before it failes',
               existing_nullable=True)
    op.alter_column('airflow_export_dags', 'use_python_dbimport',
               existing_type=mysql.TINYINT(display_width=4),
               comment='Legacy use only. Always put this to 1',
               existing_nullable=False,
               existing_server_default=sa.text("'1'"))
    op.create_table_comment(
        'airflow_export_dags',
        'To create a DAG in Airflow for Exports, this is the table that holds all definitions of the DAG configuration, including the filter that defines what tables to export, schedules times, pool names and much more. ',
        existing_comment=None,
        schema=None
    )
    op.add_column('airflow_import_dags', sa.Column('auto_regenerate_dag', mysql.TINYINT(display_width=4), server_default=sa.text("'1'"), nullable=False, comment='1 = The DAG will be auto regenerated by manage command'))
    op.add_column('airflow_import_dags', sa.Column('run_import_and_etl_separate', mysql.TINYINT(display_width=4), server_default=sa.text("'0'"), nullable=False, comment='1 = The Import and ETL phase will run in separate Tasks. '))
    op.alter_column('airflow_import_dags', 'auto_table_discovery',
               existing_type=mysql.TINYINT(display_width=4),
               comment='<NOT USED>',
               existing_nullable=False,
               existing_server_default=sa.text("'1'"))
    op.alter_column('airflow_import_dags', 'finish_all_stage1_first',
               existing_type=mysql.TINYINT(display_width=4),
               comment='1 = All Import phase jobs will be completed first, and when all is successfull, the ETL phase start',
               existing_nullable=False,
               existing_server_default=sa.text("'0'"))
    op.alter_column('airflow_import_dags', 'pool_stage1',
               existing_type=mysql.VARCHAR(length=256),
               comment='Airflow pool used for stage1 tasks. NULL for the default Hostname pool',
               existing_comment='Airflow pool used for stage1 tasks. NULL for default pool',
               existing_nullable=True)
    op.alter_column('airflow_import_dags', 'pool_stage2',
               existing_type=mysql.VARCHAR(length=256),
               comment='Airflow pool used for stage2 tasks. NULL for the default DAG pool',
               existing_comment='Airflow pool used for stage2 tasks. NULL for default pool',
               existing_nullable=True)
    op.alter_column('airflow_import_dags', 'retries',
               existing_type=mysql.TINYINT(display_width=4),
               comment='How many retries should be Task do in Airflow before it failes',
               existing_nullable=False,
               existing_server_default=sa.text("'5'"))
    op.alter_column('airflow_import_dags', 'retries_stage1',
               existing_type=mysql.TINYINT(display_width=4),
               comment='Specific retries number for Import Phase',
               existing_nullable=True)
    op.alter_column('airflow_import_dags', 'retries_stage2',
               existing_type=mysql.TINYINT(display_width=4),
               comment='Specific retries number for ETL Phase',
               existing_nullable=True)
    op.alter_column('airflow_import_dags', 'use_python_dbimport',
               existing_type=mysql.TINYINT(display_width=4),
               comment='Legacy use only. Always put this to 1',
               existing_nullable=False,
               existing_server_default=sa.text("'1'"))
    op.create_table_comment(
        'airflow_import_dags',
        'To create a DAG in Airflow for Imports, this is the table that holds all definitions of the DAG configuration, including the filter that defines what tables to import, schedules times, pool names and much more.',
        existing_comment=None,
        schema=None
    )
    op.add_column('airflow_tasks', sa.Column('sensor_connection', sa.String(length=64), nullable=True, comment='Name of Connection in Airflow'))
    op.add_column('airflow_tasks', sa.Column('sensor_poke_interval', sa.Integer(), nullable=True, comment='Poke interval for sensors in seconds'))
    op.add_column('airflow_tasks', sa.Column('sensor_timeout_minutes', sa.Integer(), nullable=True, comment='Timeout for sensors in minutes'))
    op.alter_column('airflow_tasks', 'airflow_pool',
               existing_type=mysql.VARCHAR(length=64),
               comment='Airflow Pool to use.',
               existing_nullable=True)
    op.alter_column('airflow_tasks', 'airflow_priority',
               existing_type=mysql.TINYINT(display_width=4),
               comment='Airflow Priority. Higher number, higher priority',
               existing_nullable=True)
    op.alter_column('airflow_tasks', 'dag_name',
               existing_type=mysql.VARCHAR(length=64),
               comment='Name of DAG to add Tasks to',
               existing_nullable=False)
    op.alter_column('airflow_tasks', 'hive_db',
               existing_type=mysql.VARCHAR(length=256),
               comment='<NOT USED>',
               existing_nullable=True)
    op.alter_column('airflow_tasks', 'include_in_airflow',
               existing_type=mysql.TINYINT(display_width=4),
               comment='Enable or disable the Task in the DAG during creation of DAG file.',
               existing_nullable=False,
               existing_server_default=sa.text("'1'"))
    op.alter_column('airflow_tasks', 'placement',
               existing_type=mysql.ENUM('before main', 'after main', 'in main'),
               comment='Placement for the Task',
               existing_nullable=False,
               existing_server_default=sa.text("'after main'"))
    op.alter_column('airflow_tasks', 'task_config',
               existing_type=mysql.VARCHAR(length=256),
               comment='The configuration for the Task. Depends on what Task type it is.',
               existing_nullable=True)
    op.alter_column('airflow_tasks', 'task_dependency_in_main',
               existing_type=mysql.VARCHAR(length=256),
               comment='If placement is In Main, this defines a dependency for the Task. Comma separated list',
               existing_nullable=True)
    op.alter_column('airflow_tasks', 'task_name',
               existing_type=mysql.VARCHAR(length=64),
               comment='Name of the Task in Airflow',
               existing_nullable=False)
    op.alter_column('airflow_tasks', 'task_type',
               existing_type=mysql.ENUM('shell script', 'Hive SQL Script', 'JDBC SQL'),
               comment='The type of the Task',
               existing_nullable=False,
               existing_server_default=sa.text("'Hive SQL Script'"))
    op.create_table_comment(
        'airflow_tasks',
        'All DAGs can be customized by adding Tasks into the DAG. Depending on what placement and type of Tasks that is created, DBImport will add custom placeholders to keep the DAG separated in three different parts. Before, In and After Main. In main is where all regular Imports, export or ETL jobs are executed. If you want to execute something before these, you place it in Before Main. And if you want to execute something after, you place it in After main. Please check the Airflow Integration part of the documentation for more examples and better understanding of the data you can put into this table',
        existing_comment=None,
        schema=None
    )
    op.drop_constraint('airflow_tasks_ibfk_1', 'airflow_tasks', type_='foreignkey')
    op.create_foreign_key('FK_airflow_tasks_jdbc_connections', 'airflow_tasks', 'jdbc_connections', ['jdbc_dbalias'], ['dbalias'])
    op.add_column('export_tables', sa.Column('airflow_priority', mysql.TINYINT(display_width=4), nullable=True, comment='This will set priority_weight in Airflow'))
    op.alter_column('export_tables', 'hive_javaheap',
               existing_type=mysql.BIGINT(display_width=20),
               comment='Heap size for Hive',
               existing_nullable=True)
    op.alter_column('export_tables', 'table_id',
               existing_type=mysql.INTEGER(display_width=11),
               comment='Unique identifier of the table',
               existing_nullable=False,
               autoincrement=True)
    op.alter_column('import_tables', 'table_id',
               existing_type=mysql.INTEGER(display_width=11),
               comment='Unique identifier',
               existing_nullable=False,
               autoincrement=True)
    # ### end Alembic commands ###


def downgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    op.alter_column('import_tables', 'table_id',
               existing_type=mysql.INTEGER(display_width=11),
               comment=None,
               existing_comment='Unique identifier',
               existing_nullable=False,
               autoincrement=True)
    op.alter_column('export_tables', 'table_id',
               existing_type=mysql.INTEGER(display_width=11),
               comment=None,
               existing_comment='Unique identifier of the table',
               existing_nullable=False,
               autoincrement=True)
    op.alter_column('export_tables', 'hive_javaheap',
               existing_type=mysql.BIGINT(display_width=20),
               comment=None,
               existing_comment='Heap size for Hive',
               existing_nullable=True)
    op.drop_column('export_tables', 'airflow_priority')
    op.drop_table_comment(
        'airflow_tasks',
        existing_comment='All DAGs can be customized by adding Tasks into the DAG. Depending on what placement and type of Tasks that is created, DBImport will add custom placeholders to keep the DAG separated in three different parts. Before, In and After Main. In main is where all regular Imports, export or ETL jobs are executed. If you want to execute something before these, you place it in Before Main. And if you want to execute something after, you place it in After main. Please check the Airflow Integration part of the documentation for more examples and better understanding of the data you can put into this table',
        schema=None
    )
    op.alter_column('airflow_tasks', 'task_type',
               existing_type=mysql.ENUM('shell script', 'Hive SQL Script', 'JDBC SQL'),
               comment=None,
               existing_comment='The type of the Task',
               existing_nullable=False,
               existing_server_default=sa.text("'Hive SQL Script'"))
    op.alter_column('airflow_tasks', 'task_name',
               existing_type=mysql.VARCHAR(length=64),
               comment=None,
               existing_comment='Name of the Task in Airflow')
    op.alter_column('airflow_tasks', 'task_dependency_in_main',
               existing_type=mysql.VARCHAR(length=256),
               comment=None,
               existing_comment='If placement is In Main, this defines a dependency for the Task. Comma separated list',
               existing_nullable=True)
    op.alter_column('airflow_tasks', 'task_config',
               existing_type=mysql.VARCHAR(length=256),
               comment=None,
               existing_comment='The configuration for the Task. Depends on what Task type it is.',
               existing_nullable=True)
    op.alter_column('airflow_tasks', 'placement',
               existing_type=mysql.ENUM('before main', 'after main', 'in main'),
               comment=None,
               existing_comment='Placement for the Task',
               existing_nullable=False,
               existing_server_default=sa.text("'after main'"))
    op.alter_column('airflow_tasks', 'include_in_airflow',
               existing_type=mysql.TINYINT(display_width=4),
               comment=None,
               existing_comment='Enable or disable the Task in the DAG during creation of DAG file.',
               existing_nullable=False,
               existing_server_default=sa.text("'1'"))
    op.alter_column('airflow_tasks', 'hive_db',
               existing_type=mysql.VARCHAR(length=256),
               comment=None,
               existing_comment='<NOT USED>',
               existing_nullable=True)
    op.alter_column('airflow_tasks', 'dag_name',
               existing_type=mysql.VARCHAR(length=64),
               comment=None,
               existing_comment='Name of DAG to add Tasks to')
    op.alter_column('airflow_tasks', 'airflow_priority',
               existing_type=mysql.TINYINT(display_width=4),
               comment=None,
               existing_comment='Airflow Priority. Higher number, higher priority',
               existing_nullable=True)
    op.alter_column('airflow_tasks', 'airflow_pool',
               existing_type=mysql.VARCHAR(length=64),
               comment=None,
               existing_comment='Airflow Pool to use.',
               existing_nullable=True)
    op.drop_column('airflow_tasks', 'sensor_timeout_minutes')
    op.drop_column('airflow_tasks', 'sensor_poke_interval')
    op.drop_column('airflow_tasks', 'sensor_connection')
    op.drop_constraint('FK_airflow_tasks_jdbc_connections', 'airflow_tasks', type_='foreignkey')
    op.create_foreign_key('airflow_tasks_ibfk_1', 'airflow_tasks', 'jdbc_connections', ['jdbc_dbalias'], ['dbalias'])
    op.drop_table_comment(
        'airflow_import_dags',
        existing_comment='To create a DAG in Airflow for Imports, this is the table that holds all definitions of the DAG configuration, including the filter that defines what tables to import, schedules times, pool names and much more.',
        schema=None
    )
    op.alter_column('airflow_import_dags', 'use_python_dbimport',
               existing_type=mysql.TINYINT(display_width=4),
               comment=None,
               existing_comment='Legacy use only. Always put this to 1',
               existing_nullable=False,
               existing_server_default=sa.text("'1'"))
    op.alter_column('airflow_import_dags', 'retries_stage2',
               existing_type=mysql.TINYINT(display_width=4),
               comment=None,
               existing_comment='Specific retries number for ETL Phase',
               existing_nullable=True)
    op.alter_column('airflow_import_dags', 'retries_stage1',
               existing_type=mysql.TINYINT(display_width=4),
               comment=None,
               existing_comment='Specific retries number for Import Phase',
               existing_nullable=True)
    op.alter_column('airflow_import_dags', 'retries',
               existing_type=mysql.TINYINT(display_width=4),
               comment=None,
               existing_comment='How many retries should be Task do in Airflow before it failes',
               existing_nullable=False,
               existing_server_default=sa.text("'5'"))
    op.alter_column('airflow_import_dags', 'pool_stage2',
               existing_type=mysql.VARCHAR(length=256),
               comment='Airflow pool used for stage2 tasks. NULL for default pool',
               existing_comment='Airflow pool used for stage2 tasks. NULL for the default DAG pool',
               existing_nullable=True)
    op.alter_column('airflow_import_dags', 'pool_stage1',
               existing_type=mysql.VARCHAR(length=256),
               comment='Airflow pool used for stage1 tasks. NULL for default pool',
               existing_comment='Airflow pool used for stage1 tasks. NULL for the default Hostname pool',
               existing_nullable=True)
    op.alter_column('airflow_import_dags', 'finish_all_stage1_first',
               existing_type=mysql.TINYINT(display_width=4),
               comment=None,
               existing_comment='1 = All Import phase jobs will be completed first, and when all is successfull, the ETL phase start',
               existing_nullable=False,
               existing_server_default=sa.text("'0'"))
    op.alter_column('airflow_import_dags', 'auto_table_discovery',
               existing_type=mysql.TINYINT(display_width=4),
               comment=None,
               existing_comment='<NOT USED>',
               existing_nullable=False,
               existing_server_default=sa.text("'1'"))
    op.drop_column('airflow_import_dags', 'run_import_and_etl_separate')
    op.drop_column('airflow_import_dags', 'auto_regenerate_dag')
    op.drop_table_comment(
        'airflow_export_dags',
        existing_comment='To create a DAG in Airflow for Exports, this is the table that holds all definitions of the DAG configuration, including the filter that defines what tables to export, schedules times, pool names and much more. ',
        schema=None
    )
    op.alter_column('airflow_export_dags', 'use_python_dbimport',
               existing_type=mysql.TINYINT(display_width=4),
               comment=None,
               existing_comment='Legacy use only. Always put this to 1',
               existing_nullable=False,
               existing_server_default=sa.text("'1'"))
    op.alter_column('airflow_export_dags', 'retries',
               existing_type=mysql.TINYINT(display_width=4),
               comment=None,
               existing_comment='How many retries should be Task do in Airflow before it failes',
               existing_nullable=True)
    op.alter_column('airflow_export_dags', 'operator_notes',
               existing_type=mysql.TEXT(),
               comment='Free text field to write a note about the import. ',
               existing_comment='Free text field to write a note about the export. ',
               existing_nullable=True)
    op.drop_column('airflow_export_dags', 'auto_regenerate_dag')
    op.drop_table_comment(
        'airflow_etl_dags',
        existing_comment='To create a DAG in Airflow for only ETL jobs, this is the table that holds all definitions of the DAG configuration, including the filter that defines what ETL jobs to run, schedules times, pool names and much more. ',
        schema=None
    )
    op.alter_column('airflow_etl_dags', 'trigger_dag_on_success',
               existing_type=mysql.VARCHAR(length=64),
               comment=None,
               existing_comment='<NOT USED>',
               existing_nullable=True)
    op.alter_column('airflow_etl_dags', 'retries',
               existing_type=mysql.TINYINT(display_width=4),
               comment=None,
               existing_comment='How many retries should be Task do in Airflow before it failes',
               existing_nullable=True)
    op.alter_column('airflow_etl_dags', 'operator_notes',
               existing_type=mysql.TEXT(),
               comment='Free text field to write a note about the import. ',
               existing_comment='Free text field to write a note about the ETL job. ',
               existing_nullable=True)
    op.alter_column('airflow_etl_dags', 'filter_task',
               existing_type=mysql.VARCHAR(length=64),
               comment=None,
               existing_comment='Filter string for TASK in etl_jobs table',
               existing_nullable=True)
    op.alter_column('airflow_etl_dags', 'filter_target_db',
               existing_type=mysql.VARCHAR(length=256),
               comment=None,
               existing_comment='Filter string for TARGET_DB in etl_jobs table',
               existing_nullable=True)
    op.alter_column('airflow_etl_dags', 'filter_source_db',
               existing_type=mysql.VARCHAR(length=256),
               comment=None,
               existing_comment='Filter string for SOURCE_DB in etl_jobs table',
               existing_nullable=True)
    op.alter_column('airflow_etl_dags', 'filter_job',
               existing_type=mysql.VARCHAR(length=64),
               comment=None,
               existing_comment='Filter string for JOB in etl_jobs table',
               existing_nullable=False)
    op.drop_column('airflow_etl_dags', 'auto_regenerate_dag')
    op.drop_table_comment(
        'airflow_dag_sensors',
        existing_comment='This table is used only in Legacy DBImport',
        schema=None
    )
    op.drop_table_comment(
        'airflow_custom_dags',
        existing_comment='Its possible to construct a DAG that have no import, export or ETL definitions in it, but instead just Tasks from the airflow_task table. That might nbe useful to for example run custom Hive Code after an import is completed as a separate DAG. Defining a DAG in here also requires you to have at least one task in airflow_tasks defined "in main"',
        schema=None
    )
    op.alter_column('airflow_custom_dags', 'retries',
               existing_type=mysql.INTEGER(display_width=11),
               comment=None,
               existing_comment='How many retries should be Task do in Airflow before it failes',
               existing_nullable=False,
               existing_server_default=sa.text("'0'"))
    op.alter_column('airflow_custom_dags', 'operator_notes',
               existing_type=mysql.TEXT(),
               comment='Free text field to write a note about the import. ',
               existing_comment='Free text field to write a note about the custom DAG. ',
               existing_nullable=True)
    op.drop_column('airflow_custom_dags', 'auto_regenerate_dag')
    op.create_table('airflow_execution_type',
    sa.Column('executionid', mysql.INTEGER(display_width=11), autoincrement=True, nullable=False),
    sa.Column('execution_type', mysql.TEXT(), nullable=False),
    sa.PrimaryKeyConstraint('executionid'),
    mysql_default_charset='latin1',
    mysql_engine='InnoDB'
    )
    op.create_table('airflow_import_dag_execution',
    sa.Column('dag_name', mysql.VARCHAR(length=64), nullable=False),
    sa.Column('task_name', mysql.VARCHAR(length=256), nullable=False),
    sa.Column('task_config', mysql.TEXT(), nullable=False),
    sa.Column('executionid', mysql.INTEGER(display_width=11), autoincrement=False, nullable=False),
    sa.ForeignKeyConstraint(['executionid'], ['airflow_execution_type.executionid'], name='airflow_import_dag_execution_ibfk_1'),
    sa.PrimaryKeyConstraint('dag_name', 'task_name'),
    mysql_default_charset='latin1',
    mysql_engine='InnoDB'
    )
    op.create_index('FK_airflow_import_dag_execution_airflow_execution_type', 'airflow_import_dag_execution', ['executionid'], unique=False)
    op.create_table('airflow_import_task_execution',
    sa.Column('hive_db', mysql.VARCHAR(length=256), nullable=False),
    sa.Column('hive_table', mysql.VARCHAR(length=256), nullable=False),
    sa.Column('stage', mysql.TINYINT(display_width=4), autoincrement=False, nullable=False),
    sa.Column('executionid', mysql.INTEGER(display_width=11), autoincrement=False, nullable=False),
    sa.Column('task_config', mysql.TEXT(), nullable=False),
    sa.Column('task_name', mysql.TEXT(), nullable=True),
    sa.ForeignKeyConstraint(['executionid'], ['airflow_execution_type.executionid'], name='airflow_import_task_execution_ibfk_1'),
    sa.PrimaryKeyConstraint('hive_db', 'hive_table', 'stage'),
    mysql_default_charset='latin1',
    mysql_engine='InnoDB'
    )
    op.create_index('FK_airflow_import_task_execution_airflow_execution_type', 'airflow_import_task_execution', ['executionid'], unique=False)
    # ### end Alembic commands ###
