[Database]
mysql_hostname = localhost
mysql_port = 3306
mysql_database = DBImport
mysql_username = dbimport
mysql_password = dbimport

[REST_statistics]
# Should the configuration data about the imported columns from the source systems be posted to the REST interface?
post_column_data = false

# Should the import statistics be posted to the REST interface?
post_import_data = false

# HTTP Timeout
timeout = 5

#URL to REST interface
rest_endpoint = "https://localhost:8080/test"

[Kerberos]
# This setting is only used by renewKerberosTicket.sh as a simple method of generate kerberos tickets from cron
keytab=keytab.file
principal=user@REALM

[HDFS]
hdfs_address = hdfs://hadoopCluster

[Hive]
# The SqlAlchemy connection string to the Hive metadata database.
# SqlAlchemy supports many different database engine, more information
# their website
hive_metastore_alchemy_conn = mysql+pymysql://USER:PASSWORD@localhost:3306/hive_metastore

# Connection details to Hive LLAP
hostname = hiveserver.localdomain
port = 10500
kerberos_service_name = hive
kerberos_realm = REALM

# Minimum an Maximum number of buckets for ACID tables during Merge
min_buckets = 1
max_buckets = 1024

[Sqoop]
yarnqueue = default

[Airflow]
# The SqlAlchemy connection string to the Airflow database.
# SqlAlchemy supports many different database engine, more information
# their website
airflow_alchemy_conn = mysql+pymysql://USER:PASSWORD@localhost:3306/airflow

[Credentials]
# You need a private/public key in able to encrypt and decrypt the username and password for the jdbc connections
# To generate such a pair, please use the following command and then point out the path to the keys
# If the path dont start with a /, the DBIMPORT_HOME will be used as a starting point
# openssl genrsa -out dbimport_private_key.pem 8192
# openssl rsa -in dbimport_private_key.pem -out dbimport_public_key.pem -outform PEM -pubout

private_key = conf/dbimport_private_key.pem 
public_key = conf/dbimport_public_key.pem 
